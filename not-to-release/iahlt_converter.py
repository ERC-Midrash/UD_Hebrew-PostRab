import conllu
import sys
from collections import OrderedDict
from functools import cmp_to_key

def convert_sofit_to_regular(char):
    """
    Converts a Hebrew letter in sofit form to its regular form.
    
    Args:
        char (str): A single Hebrew character in sofit form.
    
    Returns:
        str: The corresponding regular form of the character. If it is not a sofit letter, returns the original character.
    """
    sofit_to_regular = {
        'ך': 'כ',
        'ם': 'מ',
        'ן': 'נ',
        'ף': 'פ',
        'ץ': 'צ'
    }
    return sofit_to_regular.get(char, char)  # Return the original char if not found
        
def convert_to_iahlt_segmentation(token_list, debug=False):
    """
    Removes phantom tokens from a CoNLL-U token list.

    Args:
        token_list (ordered dict): CoNLL-U token_list (generated by conllu.parse).
        debug (bool): If True, adds debug information to misc field tracking changes made.

    Returns:
        token_list (ordered dict): Modified token_list with phantom tokens removed.
    
    """
    cur_whole_word = ""
    cur_range = []
    decrement_cutoffs = []
    
    i = 0
    while i < len(token_list):
        token = token_list[i]
        id = token['id']
        if type(id) not in [int, tuple]:
            raise ValueError(f"Invalid token ID type: {type(id)}. Expected int or tuple.\nToken: {token}")
        if type(id) == tuple:
            cur_whole_word = token['form']
            cur_range = range(int(id[0]), int(id[2]) + 1)
            i += 1
            continue
        if id in cur_range:
            if cur_range[0] < id < cur_range[-1] and token['form'] == "ה_":
                del token_list[i]  # remove the phantom token
                decrement_cutoffs.append(id)  # mark for id decrement
                token_list[i - 1]['feats'] = token['feats'] or {}
                token_list[i - 1]['feats']['Definite'] = 'Def'  # add definite feature to the true prefix
                if debug:
                    token_list[i - 1]['misc'] = token_list[i - 1]['misc'] or {}
                    token_list[i - 1]['misc']["PhantomRemoved"] = "Yes"  # mark the true prefix as modified
                continue  # do not increment i, since we deleted current
            elif id in cur_range[:-1] and token['form'].strip("_") == "קא": # not the last token in the range, this must be an expansion of aramaic ק prefix
                token['form'] = "ק"
            elif id == cur_range[-1] - 1 and token['form'] in ["_את_", "_של_"]:  # this is indeed a phantom token
                next_token = token_list[i + 1]  # next token is the suffix
                if debug:
                    next_token['misc'] = next_token['misc'] or {}
                    next_token['misc']["PhantomRemoved"] = "Yes"
                # Remove the phantom token from the list
                del token_list[i]
                decrement_cutoffs.append(id)
                continue  # do not increment i, since we deleted current
            elif id == cur_range[-1] and token['upos'] == "PRON":
                prev_token = token_list[i - 1]  # prev token is the main part of the word
                # Note and maybe TODO: kind of by chance, this edge case (correctly) does not cover cases like בו, לו which we do want to change because there the preposition has an underscore after it, making its length 2.
                # In general I tried to avoid assuming accurate underscoring, but it works here.
                if len(prev_token['form']) < 2 or prev_token['form'] in ["כש", "מש"]: # edge case - prefixes + demonstrative pronouns (not a pronominal suffix)
                    i += 1
                    continue
                
                num_prefix_chars = sum([len(tok['form']) for tok in token_list.filter(id=lambda x: type(x) is int and x in cur_range and x < prev_token['id'])])  # count number of chars before main part of the word
                cur_whole_word = cur_whole_word[num_prefix_chars:]  # update the whole word to exclude the prefix
                # we need to shorten the main word and extract the suffix
                lex = prev_token['lemma']
                idx = find_lexeme_index(cur_whole_word, lex)
                # extract the suffix and update the main word
                prev_token['form'] = cur_whole_word[:idx]
                token['form'] = cur_whole_word[idx:]
                # indicate that the main word was modified
                if debug:
                    prev_token['misc'] = prev_token['misc'] or {}
                    prev_token['misc']["SegModified"] = "Yes"
        i += 1
                
    # do a second pass to decrement all ids greater than the collected cutoffs
    # don't forget range-ids and dependency heads!
    for token in token_list:
        id = token['id']
        if type(id) == int:
            id_decrement = sum([1 for cutoff in decrement_cutoffs if id > cutoff])
            token['id'] -= id_decrement
            if debug and id_decrement > 0:
                token['misc'] = token['misc'] or {}
                token['misc']["IndexChanged"] = "Yes"
        elif type(id) == tuple:
            range_start = int(id[0])
            range_end = int(id[2])
            range_start_decrement = sum([1 for cutoff in decrement_cutoffs if range_start > cutoff])
            range_end_decrement = sum([1 for cutoff in decrement_cutoffs if range_end > cutoff])
            token['id'] = (range_start - range_start_decrement, id[1], range_end - range_end_decrement)
            if debug and (range_start_decrement > 0 or range_end_decrement > 0):
                token['misc'] = token['misc'] or {}
                token['misc']["IndexChanged"] = "Yes"
            continue # ranges don't have a dephead
        
        dep_head_decrement = sum([1 for cutoff in decrement_cutoffs if token['head'] > cutoff])
        token['head'] -= dep_head_decrement
        if debug and dep_head_decrement > 0:
            token['misc'] = token['misc'] or {}
            token['misc']["DepHeadChanged"] = "Yes"
    
    return token_list

def find_lexeme_index(whole_word, lexeme):
    """
    Finds the index of the lexeme in the whole word.
    
    Args:
        whole_word (str): The whole word string.
        lexeme (str): The lexeme to find in the whole word.
    
    Returns:
        int: The index of the lexeme in the whole word, or -1 if not found.
    """


    """
    # Algorithm: 
    # 
    - Look for the last index of the last character of the lexeme in the whole word, not allowing it to be the last letter of the whole word. 
    - If the last character of the lexeme is heh:
        - look for tav in the whole word. If not found,
        - look for the second to last character of the lexeme in the whole word
    - If the resulting suffix would be more than one letter (i.e. the index found is less than the length of the whole word minus 1) and begins with yod, remove the yod (i.e. increase the index by 1).
    - If all fails, return -1.
    """
    def find_full_lexeme_index(whole_word, lexeme):
        lexeme_last_char = convert_sofit_to_regular(lexeme[-1])
        if lexeme_last_char == 'ה':
            # If the last character is heh, look for tav
            tav_index = whole_word[:-1].rfind('ת')
            if tav_index != -1:
                return tav_index + 1
            # If tav is not found, look for the second to last character
            if len(lexeme) > 1: 
                second_last_char = lexeme[-2]
                second_last_index = whole_word[:-1].rfind(second_last_char)
                if second_last_index != -1:
                    return second_last_index + 1
        else:
            # If the last character is not heh, just find the last character of the lexeme
            last_index = whole_word[:-1].rfind(lexeme_last_char)
            if last_index != -1:
                return last_index + 1
        return -1
    
    index = find_full_lexeme_index(whole_word, lexeme)
    # account for the possible yod
    if index != -1 and index < len(whole_word) - 1 and whole_word[index] in 'וי':
        index += 1
    return index
    


def convert_all_sentences_to_iahlt(conllu_string, debug=False):
    try:
        sentences = conllu.parse(conllu_string)
    except Exception as e:
        print(f"Error processing CoNLL-U string: {e}")
        return
        # Optionally log the error and the problematic sentence string
        # print(f"Problematic sentence:\n{conllu_string}")
    
    for sentence in sentences:
        metadata = sentence.metadata
        modified_token_list = convert_to_iahlt_segmentation(sentence, debug=debug)
        modified_token_list = conllu.TokenList(modified_token_list) # Ensure it's a TokenList object
        modified_token_list.metadata = metadata # Preserve metadata
        yield modified_token_list.serialize() # Serialize the modified token list
    
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Convert CoNLL-U file to IAHLT segmentation by removing phantom tokens.")
    parser.add_argument("-i", "--input", help="Path to the input CoNLL-U file.", required=True)
    parser.add_argument("-o", "--output", help="Path to the output CoNLL-U file.", required=True)
    parser.add_argument("-d", "--debug", help="Enable debug mode.", action="store_true", default=False)
    args = parser.parse_args()

    with open(args.input, "r", encoding="utf-8") as infile, \
         open(args.output, "w", encoding="utf-8", newline='\n') as outfile:
        conllu_string = infile.read()
        modified_conllu = convert_all_sentences_to_iahlt(conllu_string, debug=args.debug)
        for sentence in modified_conllu:
            outfile.write(sentence)
            # outfile.write("\n")

           

